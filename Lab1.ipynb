{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8_gE0cxQ1Ix"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "Dominykas Misius, 2213772\n",
        "\n",
        "**Model:** ResNet50\n",
        "\n",
        "**Dataset:** Open Images V7\n",
        "\n",
        "**Classes:** Cat, Dog, Fruit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA9iHg9CW6Gs"
      },
      "outputs": [],
      "source": [
        "!pip install -q fiftyone\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Mount Google Drive to persist data between sessions\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/GMM_Lab1'\n",
        "LABELS_PATH = os.path.join(BASE_DIR, 'ground_truth_labels.json')\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "print(f\"Base dir: {BASE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVAJu7XUQ1I0"
      },
      "source": [
        "## 2. Data Download from OpenImages V7\n",
        "\n",
        "Downloading 1000 validation images using FiftyOne. Labels are saved to a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-86ExplPX-8E"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "CHOSEN_CLASSES = [\"Cat\", \"Dog\", \"Fruit\"]\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"open-images-v6\",\n",
        "    split=\"validation\",\n",
        "    max_samples=1000,\n",
        "    classes=CHOSEN_CLASSES,\n",
        "    label_types=[\"classifications\"],\n",
        ")\n",
        "\n",
        "DATA_DIR = os.path.dirname(next(iter(dataset)).filepath)\n",
        "print(f\"Images at: {DATA_DIR}\")\n",
        "\n",
        "if os.path.exists(LABELS_PATH):\n",
        "    print(\"Labels file found, loading from disk.\")\n",
        "    with open(LABELS_PATH, 'r') as f:\n",
        "        ground_truth = json.load(f)\n",
        "else:\n",
        "    ground_truth = {}\n",
        "    for sample in dataset:\n",
        "        filename = os.path.basename(sample.filepath)\n",
        "        pos_labels = set()\n",
        "        if sample.positive_labels is not None:\n",
        "            for cls in sample.positive_labels.classifications:\n",
        "                if cls.label in CHOSEN_CLASSES:\n",
        "                    pos_labels.add(cls.label)\n",
        "\n",
        "        ground_truth[filename] = list(pos_labels)\n",
        "\n",
        "    with open(LABELS_PATH, 'w') as f:\n",
        "        json.dump(ground_truth, f, indent=2)\n",
        "    print(f\"Labels saved to: {LABELS_PATH}\")\n",
        "\n",
        "print(f\"\\nTotal images: {len(ground_truth)}\")\n",
        "for cls in CHOSEN_CLASSES:\n",
        "    count = sum(1 for labels in ground_truth.values() if cls in labels)\n",
        "    print(f\"  {cls}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G5Mz69rQ1I2"
      },
      "source": [
        "## 3. Dataset Class and DataLoader\n",
        "\n",
        "Standard ImageNet normalization is applied to every image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtt9gMVwYBAl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\")\n",
        "\n",
        "# Map OpenImages class names to corresponding ids\n",
        "IMAGENET_MAPPING = {\n",
        "    \"Cat\":   list(range(281, 286)),\n",
        "    \"Dog\":   list(range(151, 269)),\n",
        "    \"Fruit\": list(range(948, 958)),\n",
        "}\n",
        "\n",
        "IMAGENET_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class OpenImagesDataset(Dataset):\n",
        "    def __init__(self, data_dir, labels_dict, chosen_classes):\n",
        "        self.image_paths = sorted([\n",
        "            os.path.join(data_dir, f)\n",
        "            for f in os.listdir(data_dir)\n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ])\n",
        "        self.labels_dict = labels_dict\n",
        "        self.chosen_classes = chosen_classes\n",
        "        print(f\"Dataset: {len(self.image_paths)} images from {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image_tensor = IMAGENET_TRANSFORM(Image.open(img_path).convert(\"RGB\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {img_path}: {e}\")\n",
        "            image_tensor = torch.zeros(3, 224, 224)\n",
        "\n",
        "        filename = os.path.basename(img_path)\n",
        "        gt_labels = self.labels_dict.get(filename, [])\n",
        "        binary_labels = torch.tensor(\n",
        "            [1.0 if cls in gt_labels else 0.0 for cls in self.chosen_classes],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        return image_tensor, binary_labels\n",
        "\n",
        "\n",
        "val_dataset = OpenImagesDataset(\n",
        "    data_dir=DATA_DIR,\n",
        "    labels_dict=ground_truth,\n",
        "    chosen_classes=list(IMAGENET_MAPPING.keys()),\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"DataLoader: {len(val_loader)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9MZoxaxQ1I2"
      },
      "source": [
        "## 4. Model Loading and Inference\n",
        "\n",
        "Load ResNet50, run all images through it, store the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGX3DamqYDDc"
      },
      "outputs": [],
      "source": [
        "print(\"Loading pretrained ResNet50...\")\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "all_probs, all_labels = [], []\n",
        "print(f\"Running inference on {len(val_dataset)} images...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Inference\"):\n",
        "        p = torch.softmax(model(images.to(DEVICE, non_blocking=True)), dim=1)\n",
        "        all_probs.append(p.cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "probs = np.vstack(all_probs)\n",
        "true_labels = np.vstack(all_labels)\n",
        "print(f\"Done. Probs: {probs.shape}, Labels: {true_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8jpfrTQ1I3"
      },
      "source": [
        "## 5. Metrics with Variable Threshold\n",
        "\n",
        "An image is marked positive if the models probability is at least T\n",
        "\n",
        "- **Accuracy** — how many images were classified correctly\n",
        "- **Precision** — of all images the model said were positive, how many actually were\n",
        "- **Recall** — of all images that actually were positive, how many did the model find\n",
        "- **F1** — balance between precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPOE6VGqdhk1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "CLASS_NAMES = list(IMAGENET_MAPPING.keys())\n",
        "\n",
        "\n",
        "def compute_and_display_metrics(threshold):\n",
        "    print(f\"Threshold T = {threshold:.2f}\\n\")\n",
        "\n",
        "    for i, cls_name in enumerate(CLASS_NAMES):\n",
        "        imagenet_ids = IMAGENET_MAPPING[cls_name]\n",
        "\n",
        "        # Max probability across all matching ImageNet IDs\n",
        "        class_probs = probs[:, imagenet_ids].max(axis=1)\n",
        "        predicted = (class_probs >= threshold).astype(int)\n",
        "        true_binary = true_labels[:, i].astype(int)\n",
        "\n",
        "        acc  = accuracy_score(true_binary, predicted)\n",
        "        prec = precision_score(true_binary, predicted, zero_division=0)\n",
        "        rec  = recall_score(true_binary, predicted, zero_division=0)\n",
        "        f1   = f1_score(true_binary, predicted, zero_division=0)\n",
        "\n",
        "        print(f\"  --- {cls_name} ---\")\n",
        "        print(f\"  GT positives:  {true_binary.sum()}\")\n",
        "        print(f\"  Predicted pos: {predicted.sum()}\")\n",
        "        print(f\"  Accuracy:      {acc:.4f}\")\n",
        "        print(f\"  Precision:     {prec:.4f}\")\n",
        "        print(f\"  Recall:        {rec:.4f}\")\n",
        "        print(f\"  F1:            {f1:.4f}\\n\")\n",
        "\n",
        "widgets.interact(\n",
        "    compute_and_display_metrics,\n",
        "    threshold=widgets.FloatSlider(\n",
        "        value=0.5, min=0.0, max=1.0, step=0.01,\n",
        "        description='T:',\n",
        "        style={'description_width': 'initial'},\n",
        "        continuous_update=False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGy5vmP-Q1I5"
      },
      "source": [
        "## 6. Custom Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd_p2EYQQ1I5"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import urllib.request\n",
        "from google.colab import files\n",
        "from IPython.display import Image as IPImage\n",
        "\n",
        "# Fetch ImageNet class names\n",
        "_labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "with urllib.request.urlopen(_labels_url) as r:\n",
        "    imagenet_class_names = [line.strip() for line in r.read().decode().splitlines()]\n",
        "\n",
        "print(\"Upload an image:\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"No images uploaded.\")\n",
        "else:\n",
        "    for filename, file_bytes in uploaded_files.items():\n",
        "        print(f\"\\n--- {filename} ---\")\n",
        "        display(IPImage(data=file_bytes, width=300))\n",
        "\n",
        "        img_tensor = IMAGENET_TRANSFORM(\n",
        "            Image.open(io.BytesIO(file_bytes)).convert(\"RGB\")\n",
        "        ).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs_single = torch.softmax(model(img_tensor), dim=1).cpu().numpy()[0]\n",
        "\n",
        "        print(\"\\n  Top-5 predictions:\")\n",
        "        for rank, idx in enumerate(probs_single.argsort()[::-1][:5], 1):\n",
        "            print(f\"    {rank}. {imagenet_class_names[idx]:<30s}  {probs_single[idx]:.4f}\")\n",
        "\n",
        "        print(\"\\n  Our class probabilities:\")\n",
        "        for cls_name, imagenet_ids in IMAGENET_MAPPING.items():\n",
        "            cls_prob = probs_single[imagenet_ids].max()\n",
        "            best_id  = imagenet_ids[probs_single[imagenet_ids].argmax()]\n",
        "            print(f\"    {cls_name:<8s}  {cls_prob:.4f}  (best match: {imagenet_class_names[best_id]})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}